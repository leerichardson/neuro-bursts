\documentclass[11pt]{article} 

\usepackage[titlenumbered,ruled]{algorithm2e} % Algorithms
\usepackage[margin=1.2in]{geometry}% Changing the Margins 
\usepackage{natbib} % Bibliography
\usepackage{graphicx} % Figures
\usepackage{booktabs}% For Cross-Tab Titles
\usepackage{url} % Links 
\usepackage{hyperref}

% Define mathematical functions 
\usepackage{amsmath,amsthm,amssymb} % Mathematics 
\usepackage{bbold} % Indicator Functions 
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% From Alex's Thesis Template 
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\p}{\mathbb{P}}
\DeclareMathOperator{\var}{\mathbf{Var}}
\DeclareMathOperator{\cov}{\mathbf{Cov}}
\DeclareMathOperator{\ccf}{\mathbf{CCF}}
\DeclareMathOperator{\cor}{\mathbf{Cor}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand\floor[1]{\lfloor#1\rfloor}

\begin{document}

\title{{\Large A Statistical Model for Burst Detection}}
\author{Lee F. Richardson}
\date{\today}
\maketitle

\section{Introduction}
This document proposes a statistical model and estimation procedure for to detect ``Bursts'' in Local Field Potential (LFP) signals. By bursts, I mean local (in time) oscillations at particular frequencies. The motivation for burst detection comes from \cite{lundqvist2016gamma}, who finds that bursts, {\it not} sustained oscillations, underly working memory. 

\section{Literature Review}
\label{sec:lit-review}

\section{Data}
\label{sec:data}

\section{A Statistical Model for Bursts}
\label{sec:model}

\begin{figure}[!ht]
  \centering
  \includegraphics[width = 15cm, scale=1]{images/sample-burst.png}
  \caption{Top: An example Local Field Potential Signal in out data-set that displays ``bursting'' behavior. The red line is a trend-filter estimate of the slow trend in the signal, which we remove for spectral analysis. Bottom: Multi-taper SWDFT of the de-trended LFP signal. We see bursting behavior around 40 Hz, which corresponds to the ``fixation'' part of the trial.}
\label{fig:example-burst}
\end{figure}

A graphical example of a burst in our data-set is shown in Figure \ref{fig:example-burst}. Figure \ref{fig:example-burst} shows that, for a short time window, the LFP signal has oscillations in the 40-60 Hz range. This short oscillation is what we refer to as a ``Burst''. This observation suggests a simple deterministic model for a ``burst'' in our LFP signal

\begin{eqnarray}
	\label{eq:deterministic}
	g_t &=& A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S + L - 1}(t)
\end{eqnarray}

\noindent Which is simply a cosine function multiplied by a box-car window. Equation \ref{eq:deterministic} has $5$ parameters:

\begin{itemize}
\setlength\itemsep{.1em}
	\item $A$: Amplitude. $A \in [0, \infty]$
	\item $F$: Frequency. Number of cycles in a length $N$ signal. $F \in [0, \frac{1}{2}]$
	\item $\phi$: Phase. $\phi \in [0, 2 \pi]$
	\item $S$: Start of oscillation. $S \in [0, 1, \ldots, N - 2]$
	\item $L$: Length of oscillation. $L \in [1, 2, \ldots, N - 2]$
\end{itemize}

\noindent We turn Equation \ref{eq:deterministic} into a statistical model by introducing an error term;

\begin{eqnarray}
\label{eq:statistical-model}
x_t &=& g_t + \epsilon_t 
\end{eqnarray}

\noindent To keep things simple, assume $\epsilon_t \sim N(0, \sigma^2)$ and $\epsilon_t$ is i.i.d.

\section{Estimation}
\label{sec:estimation}


Now that we've proposed a statistical model(Equation \ref{eq:statistical-model}), given an arbitrary signal, we need to estimate the parameters. Section \ref{sec:analytic} derives analytic solutions for three of the six parameters (A, $\phi$, and $\sigma$), using a standard trick from harmonic regression. Section \ref{sec:numericalslf} shows how to solve for the remaining three parameters (S, L, and F) numerically, using a Spectrogram. 

\subsection{Estimating $A, \phi$, and $\sigma$}
\label{sec:aphisigma}

Following Equation \ref{eq:statistical-model}, we know that $x_t \sim N(g_t, \sigma^2)$. Denote $\theta = [A, F, \phi, S, L, \sigma]$ as our vector of unknown parameters, and say that we observe a length $N$ time-series ${\bf x} = [x_0, x_1, \ldots, x_{N - 1}]$. The joint pdf of {\bf x} is:

\begin{eqnarray}
\label{eq:jointpdf}
	\mathbb{P}(x_0, \ldots, x_{N - 1} | \theta) &=& \prod_{t=0}^{N-1} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(\frac{-(x_t - g_t)^2}{2 \sigma^2})
\end{eqnarray}

\noindent Therefore, the likelihood function is:

\begin{eqnarray}
\label{eq:likelihood}
	L(\theta | {\bf x}) &=& \prod_{t=0}^{N-1} \frac{1}{\sqrt{2 \pi} \sigma} \exp(\frac{-(x_t - g_t)^2}{2 \sigma^2})
\end{eqnarray}

\noindent And the log-likelihood is proportional to

\begin{eqnarray}
\label{eq:loglikelihood}
	\ell(\theta) &\propto& -N \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{t=0}^{N-1} (x_t - g_t)^2
\end{eqnarray}

\noindent The maximum likelihood estimate is:

\begin{eqnarray}
	\hat{\theta}_{ML} &=& \argmax_{\theta} \ell(\theta)
\end{eqnarray}

This section assumes that S, L, and F are known, and we will show how to solve for them numerically in the next section. Then we have

\begin{eqnarray}
	\ell(A, \phi, \sigma | S, L, F) &\propto& -N \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{t=0}^{N-1} (x_t - A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

Next, notice that for any value of $\sigma$, the log-likelihood is {\it maximized} when the quadratic term inside the sum is minimized (since there is a negative sign in front of the sum). With this, we closely follow the second example of Section 7.10 of \cite{kay1993fundamentals}, and let $J(A, \phi)$ be the term inside the sum we want to minimize:

\begin{eqnarray}
	J(A, \phi) &=& \sum_{t=0}^{N-1} (x_{t} - A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

\noindent Which looks like least squares. The only problem is that the cosine factor is non-linear, but we can linearize it with the following trigonometric identity:

\begin{eqnarray}
	A \cos(\frac{2 \pi F t}{N} + \phi) &=& A\cos(\phi)\cos(\frac{2 \pi F t}{N})  - A \sin(\phi) \sin(\frac{2 \pi F t}{N})
\end{eqnarray}

\noindent Then if we define the following coefficients:

\begin{eqnarray}
	\beta_1 &=& A \cos(\phi) \nonumber \\
	\beta_2 &=& -A \sin(\phi)
\end{eqnarray}

\noindent There is a one-to-one correspondence between $(\beta_1, \beta_2)$ and $(A, \phi)$:

\begin{eqnarray}
\label{eq:aptobeta}
	A &=& \sqrt{\beta_1^2 + \beta_2^2} \nonumber \\
	\phi &=& \arctan(\frac{-\beta_2}{\beta_1})
\end{eqnarray}

\noindent We can re-write $J(A, \phi)$ instead as a function of the $\beta$'s:

\begin{eqnarray}
\label{eq:localbetas}
	J(\beta_1, \beta_2) &=& \sum_{t=0}^{N-1} (x_t - \beta_1 \cos(\frac{2 \pi F t}{N})  \mathbb{1}_{S, S+L-1}(t) - \beta_2 \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

Equation \ref{eq:localbetas} is just multiple linear regression, which means we can solve for $\beta$ analytically. We now write $J(\beta_1, \beta_2)$ in matrix-vector notation. Define the following

\begin{eqnarray}
	{\bf c} &=& [\cos(\frac{2 \pi F \cdot 0}{N}) \mathbb{1}_{S,S+L-1}(0),  \cos(\frac{2 \pi F \cdot 1}{N}) \mathbb{1}_{S,S+L-1}(1), \ldots \cos(\frac{2 \pi F \cdot (N-1)}{N}) \mathbb{1}_{S,S+L-1}(N-1)]^T \nonumber \\
	{\bf s} &=& [\sin(\frac{2 \pi F \cdot 0}{N}) \mathbb{1}_{S,S+L-1}(0),  \sin(\frac{2 \pi F \cdot 1}{N}) \mathbb{1}_{S,S+L-1}(1), \ldots \sin(\frac{2 \pi F \cdot (N-1)}{N}) \mathbb{1}_{S,S+L-1}(N-1)]^T \nonumber \\
	{\bf U} &=& [{\bf c}, {\bf s}] \nonumber \\
	{\bf x} &=& [x_0, x_1, \ldots, x_{N-1}] \nonumber \\
	\beta &=& [\beta_1, \beta_2]
\end{eqnarray}

\noindent $J(\beta_1, \beta_2)$ can now be written as:

\begin{eqnarray}
	\label{eq:sumsquares}
	J(\beta_1, \beta_2) &=& ({\bf x} - {\bf U} {\bf \beta})^T ({\bf x} - {\bf U} {\bf \beta}) 
\end{eqnarray}

\noindent And the estimate for $\hat{\beta}$ is:

\begin{eqnarray}
\label{eq:betahat}
	\hat{\beta} &=& ({\bf U}^T {\bf U})^{-1} {\bf U}^T {\bf x} 
\end{eqnarray}

\noindent Which we transform into $\hat{A}$ and $\hat{\phi}$ using Equation \ref{eq:aptobeta}. For $\sigma$, we have:

\begin{eqnarray}
	\frac{\partial \ell}{\partial \sigma} &=& \frac{-N}{\sigma} + \frac{1}{\sigma^3} \sum_{t=S}^{S+L-1} (x_t - g_t)^2
\end{eqnarray}

\noindent Setting this to $0$ and plugging in $\hat{A}$ and $\hat{\phi}$ gives:

\begin{eqnarray}
	\hat{\sigma} &=& \sqrt{\frac{1}{N} \sum_{t=0}^{N-1} (x_t - g_t)^2}
\end{eqnarray}

\noindent Similar to standard estimates of $\sigma$ with normal errors. 	

So, if we assume $S$, $L$, and $F$ are known, we can estimate $\hat{A}$, $\hat{\phi}$, and $\hat{\sigma}$ analytically. But in practice, we don't know the values of $S$, $L$, and $F$, and we need to estimate them as well. This is where we use the Spectrogram. 

\subsection{Estimating S, L, and F}
\label{sec:slf}
Section \ref{sec:aphisigma} shows that, if we {\it know} S, L, and F, we have analytic solutions to the maximum likelihood estimates of A, $\phi$, and $\sigma$. So the next question is, how can we estimate S, L, and F? Recall the sum of squares term we minimized in Equation \ref{eq:sumsquares}

\begin{eqnarray}
	J(\hat{\beta_1}, \hat{\beta_2}) &=& ({\bf x} - {\bf U} \hat{\beta})^T ({\bf x} - {\bf U} \hat{\beta})
\end{eqnarray}

\noindent Since {\bf U} includes S, L, and F, we can extend this minimization to these three parameters

\begin{eqnarray}
\label{eq:extendmin}
	J(\hat{\beta_1}, \hat{\beta_2}, S, L, F) &=& ({\bf x} - {\bf U} \hat{\beta})^T ({\bf x} - {\bf U} \hat{\beta})
\end{eqnarray}


\noindent Plugging in Equation \ref{eq:betahat} gives (\cite{kay1993fundamentals} Equation 7.65):

\begin{eqnarray}
	J(\hat{\beta_1}, \hat{\beta_2}, S, L, F) &=& {\bf x}^T ({\bf I} - {\bf U} ({\bf U}^T {\bf U})^{-1} {\bf U}^T) {\bf x}
\end{eqnarray}

\noindent Which is equivalent to finding the values of S, L, and F that maximize

\begin{eqnarray}
	\argmax_{S, L, F} && {\bf x}^{T} {\bf U} ({\bf U}^T {\bf U})^{-1} {\bf U}^T {\bf x}
\end{eqnarray}

\noindent Since ${\bf U} = [{\bf c}, {\bf s}]$, we can re-write this maximization as

$$
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}^T
\begin{bmatrix}
{\bf c}^T {\bf c} & {\bf c}^T {\bf s} \\ 
{\bf s}^T {\bf c} & {\bf s}^T {\bf s}
\end{bmatrix}^{-1}
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}
$$

\noindent We can approximate the diagonal terms in the $2 \times 2$ matrix as

\begin{eqnarray}
	{\bf c}^T {\bf c} &=& \sum_{t=0}^{N-1} \cos(\frac{2 \pi F t}{N})^2 \mathbb{1}_{S, S+L-1}(t) \approx \frac{L}{2} \nonumber \\
	{\bf s}^T {\bf s} &=& \sum_{t=0}^{N-1} \sin(\frac{2 \pi F t}{N})^2 \mathbb{1}_{S, S+L-1}(t) \approx \frac{L}{2} \nonumber 
\end{eqnarray}

\noindent For the off diagonal terms, we have:

\begin{eqnarray}
\label{eq:offdiags}
	{\bf c}^T {\bf s} &=& {\bf s}^T {\bf c} \nonumber \\
	&=& \sum_{t=0}^{N-1} \cos(\frac{2 \pi F t}{N}) \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t) \nonumber \\
	&=& 2 \sum_{t=0}^{N-1} \sin(\frac{4 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t) \nonumber \\
\end{eqnarray}

\begin{figure}[!ht]
  \centering
  \includegraphics[width = 12cm, scale=1]{images/off-diagonal.png}
  \caption{Off Diagonal Terms. N=256, L=10-200, S=30, F=16,28,20. }
\label{fig:offdiagonal}
\end{figure}

\noindent And this is actually the key difference between our problem and the example in \cite{kay1993fundamentals}. In \cite{kay1993fundamentals}, since this summation is over an entire cycle, Equation \ref{eq:offdiags} is $\approx$ 0. But for our problem, we need to include this term, since it only cancels out when $L$ is a complete cycle (see Figure \ref{fig:offdiagonal} for a demonstration). So for now, let ${\bf c}^T {\bf s} = z(S, L, F, N)$, or $z$ for short. Note that we can bound $z$ in the range $[-2, 2]$. Then our expression is

$$
\approx 
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}^T
\begin{bmatrix}
\frac{L}{2} & z \\ 
z & \frac{L}{2}	
\end{bmatrix}^{-1}
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}
$$

The inverse of the inner matrix is

$$
\frac{1}{\frac{L^2}{4} - z^2}
\begin{bmatrix}
\frac{L}{2} & -z \\
-z & \frac{L}{2}
\end{bmatrix}
\approx 
\frac{4}{L^2}
\begin{bmatrix}
\frac{L}{2} & -z \\
-z & \frac{L}{2}
\end{bmatrix}
$$

\noindent Plugging this back in gives 

\begin{eqnarray}
	\frac{4}{L^2}[\frac{L}{2} ({\bf c}^T {\bf x})^2 + \frac{L}{2} ({\bf s}^T {\bf x})^2 - 2 z ({\bf c}^T {\bf x}) ({\bf s}^T {\bf x})] &=& \frac{2}{L}[({\bf c}^T {\bf x})^2 + ({\bf s}^T {\bf x})^2] - \frac{8z}{L^2} ({\bf c}^T {\bf x}) ({\bf s}^T {\bf x}) \nonumber 
\end{eqnarray}

\noindent Since the first term on the RHS is normalized by $1/L$, and the second term is normalized by $\frac{1}{L^2}$, the first term dominates this maximization (and if $z = 0$, or close to it, we know that the second term on the RHS goes away). Therefore, we focus on maximizing the first term in this expression. Notice that

\begin{eqnarray}
\label{eq:maxslfspec}
	\frac{2}{L} [ ({\bf c}^T {\bf x})^2 + ({\bf s}^T {\bf x})^2] &=& \frac{2}{L} [ (\sum_{t=0}^{N-1} x_t \cos(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t))^2 + (\sum_{t=0}^{N-1} x_t \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t))^2]  \nonumber \\
	&=& \frac{2}{L} |\sum_{t=0}^{N-1} x_t \omega_N^{-tF} \mathbb{1}_{S,S+L-1}(t)|^2 \nonumber \\
	&=& \frac{2}{L} |\sum_{t=S}^{S+L-1} x_t \omega_N^{-tF}|^2
\end{eqnarray}

Which looks strikingly similar to the Periodogram (he MLE estimate in \cite{kay1993fundamentals}). Recall the definition of the Sliding Window Discrete Fourier Transform (SWDFT), with length $n$ window size

\begin{eqnarray}
\label{eq:swdft}
	a_{k, p, n} &=& \sum_{j=0}^{n-1} x_{p-n+1+j} \omega_n^{-jk} \nonumber \\
	k &=& 0, 1, \ldots, n - 1 \nonumber \\
	p &=& n - 1, n, \ldots N - 1 
\end{eqnarray}

\noindent And if we let $\frac{f}{L} = \frac{F}{N}$, we can interpret Equation \ref{eq:maxslfspec} as a SWDFT with length $L$ windows

\begin{eqnarray}
\label{eq:optimizationslf}
	\argmax_{S, L, F} && \frac{2}{L} |\sum_{t=S}^{S+L-1} x_t \omega_L^{-tf}|^2 \nonumber 
\end{eqnarray}

\noindent Equation \ref{eq:optimizationslf} tells us that our estimates of S, L, and F will be the values that maximize the Sliding Window DFT across varying window sizes.

So in practice, how would we make these estimates? The first step would be to find the maximum squared modulus SWDFT across window sizes

\begin{eqnarray}
\label{eq:maxswdftwindow}
	(\hat{k}, \hat{p}, \hat{n}) &=&\argmax_{k,p,n} |a_{k, p, n}|^2
\end{eqnarray}

\noindent Which gives the following estimates of S, L, and F

\begin{eqnarray}
	\hat{L} &=& \hat{n} \nonumber \\
	\hat{S} &=& \hat{p} - \hat{n} + 1 \nonumber \\
	\hat{F} &=& \frac{\hat{k}}{\hat{n}}
\end{eqnarray}

\noindent The heavy computational component of this approach is repeatedly computing Spectrograms of varying window sizes. We are currently looking into methods to speed this up. Then with S, L, and F, you can easily obtain analytic solutions from Section \ref{sec:aphisigma} for A, $\phi$, and $\sigma$. If desired, further numerical optimization may be performed around the estimate of $\hat{F}$, which ensures we aren't restricting $F$ to the set of Fourier Frequencies. 

% \section{Results}
% \label{sec:results}

% \section{Discussion}
% \label{sec:discussion}

\bibliographystyle{apa}
\bibliography{references}

\end{document}