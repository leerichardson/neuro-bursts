\documentclass[11pt]{article} 

\usepackage[titlenumbered,ruled]{algorithm2e} % Algorithms
\usepackage[margin=1.2in]{geometry}% Changing the Margins 
\usepackage{natbib} % Bibliography
\usepackage{graphicx} % Figures
\usepackage{booktabs}% For Cross-Tab Titles
\usepackage{url} % Links 
\usepackage{hyperref}

% Define mathematical functions 
\usepackage{amsmath,amsthm,amssymb} % Mathematics 
\usepackage{bbold} % Indicator Functions 
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% From Alex's Thesis Template 
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\p}{\mathbb{P}}
\DeclareMathOperator{\var}{\mathbf{Var}}
\DeclareMathOperator{\cov}{\mathbf{Cov}}
\DeclareMathOperator{\ccf}{\mathbf{CCF}}
\DeclareMathOperator{\cor}{\mathbf{Cor}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand\floor[1]{\lfloor#1\rfloor}

\begin{document}

\title{{\Large A Statistical Approach to Burst Detection}}
\author{Lee F. Richardson}
\date{\today}
\maketitle

\section{Introduction}
This document proposes a statistical model and estimation procedure for to detect ``Bursts'' in Local Field Potential (LFP) signals. By bursts, I mean local (in time) oscillations at particular frequencies. The motivation for burst detection comes from \cite{lundqvist2016gamma}, who finds that bursts, {\it not} sustained oscillations, underly working memory. 

\section{Literature Review}
\label{sec:lit-review}

\section{Data}
\label{sec:data}

\section{A Statistical Model for Bursts}
\label{sec:model}

\begin{figure}[!ht]
  \centering
  \includegraphics[width = 15cm, scale=1]{images/sample-burst.png}
  \caption{Top: An example Local Field Potential Signal in out data-set that displays ``bursting'' behavior. The red line is a trend-filter estimate of the slow trend in the signal, which we remove for spectral analysis. Bottom: Multi-taper SWDFT of the de-trended LFP signal. We see bursting behavior around 40 Hz, which corresponds to the ``fixation'' part of the trial.}
\label{fig:example-burst}
\end{figure}

A graphical example of a burst in our data-set is shown in Figure \ref{fig:example-burst}. Figure \ref{fig:example-burst} shows that, for a short time window, the LFP signal has oscillations in the 40-60 Hz range. This short oscillation is what we refer to as a ``Burst''. This observation suggests a simple deterministic model for a ``burst'' in our LFP signal

\begin{eqnarray}
	\label{eq:deterministic}
	g_t &=& A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S + L - 1}(t)
\end{eqnarray}

\noindent Which is simply a cosine function multiplied by a box-car window. Equation \ref{eq:deterministic} has $5$ parameters:

\begin{itemize}
\setlength\itemsep{.1em}
	\item $A$: Amplitude. $A \in [0, \infty]$
	\item $F$: Frequency. Number of cycles in a length $N$ signal. $F \in [0, \frac{1}{2}]$
	\item $\phi$: Phase. $\phi \in [0, 2 \pi]$
	\item $S$: Start of oscillation. $S \in [0, 1, \ldots, N - 2]$
	\item $L$: Length of oscillation. $L \in [1, 2, \ldots, N - 2]$
\end{itemize}

\noindent We turn Equation \ref{eq:deterministic} into a statistical model by introducing an error term;

\begin{eqnarray}
\label{eq:statistical-model}
x_t &=& g_t + \epsilon_t 
\end{eqnarray}

\noindent To keep things simple, assume $\epsilon_t \sim N(0, \sigma^2)$ and $\epsilon_t$ is i.i.d.

\section{Estimation}
\label{sec:estimation}
Now that we've proposed a statistical model(Equation \ref{eq:statistical-model}), given an arbitrary signal, we need to estimate the parameters. Section \ref{sec:aphisigma} derives analytic solutions for three of the six parameters (A, $\phi$, and $\sigma$), using a standard trick from harmonic regression. Section \ref{sec:slf} shows how to solve for the remaining three parameters (S, L, and F) numerically, using a Spectrogram. 

\subsection{Estimating $A, \phi$, and $\sigma$}
\label{sec:aphisigma}

Following Equation \ref{eq:statistical-model}, we know that $x_t \sim N(g_t, \sigma^2)$. Denote $\theta = [A, F, \phi, S, L, \sigma]$ as our vector of unknown parameters, and say that we observe a length $N$ time-series ${\bf x} = [x_0, x_1, \ldots, x_{N - 1}]$. The joint pdf of {\bf x} is:

\begin{eqnarray}
\label{eq:jointpdf}
	\mathbb{P}(x_0, \ldots, x_{N - 1} | \theta) &=& \prod_{t=0}^{N-1} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(\frac{-(x_t - g_t)^2}{2 \sigma^2})
\end{eqnarray}

\noindent The likelihood function is

\begin{eqnarray}
\label{eq:likelihood}
	L(\theta | {\bf x}) &=& \prod_{t=0}^{N-1} \frac{1}{\sqrt{2 \pi} \sigma} \exp(\frac{-(x_t - g_t)^2}{2 \sigma^2})
\end{eqnarray}

\noindent And the log-likelihood is proportional to

\begin{eqnarray}
\label{eq:loglikelihood}
	\ell(\theta) &\propto& -N \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{t=0}^{N-1} (x_t - g_t)^2
\end{eqnarray}

\noindent The maximum likelihood estimate is

\begin{eqnarray}
	\hat{\theta}_{ML} &=& \argmax_{\theta} \ell(\theta)
\end{eqnarray}

This section assumes S, L, and F are known, and solves for A, $\phi$, and $\sigma$ (the next section shows how to estimate S, L, and F). We have

\begin{eqnarray}
	\ell(A, \phi, \sigma | S, L, F) &\propto& -N \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{t=0}^{N-1} (x_t - A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

Next, notice that for any value of $\sigma$, the log-likelihood is {\it maximized} when the quadratic term inside the sum is minimized (since there is a negative sign in front of the sum). With this, we closely follow the second example of Section 7.10 of \cite{kay1993fundamentals}, and let $J(A, \phi)$ be the term inside the sum we want to minimize

\begin{eqnarray}
	J(A, \phi) &=& \sum_{t=0}^{N-1} (x_{t} - A \cos(\frac{2 \pi F t}{N} + \phi) \cdot \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

\noindent This looks exactly like least squares, but the cosine factor is non-linear. We can linearize the cosine factor with the following trigonometric identity

\begin{eqnarray}
	A \cos(\frac{2 \pi F t}{N} + \phi) &=& A\cos(\phi)\cos(\frac{2 \pi F t}{N})  - A \sin(\phi) \sin(\frac{2 \pi F t}{N})
\end{eqnarray}

\noindent Then we define the coefficients

\begin{eqnarray}
	\beta_1 &=& A \cos(\phi) \nonumber \\
	\beta_2 &=& -A \sin(\phi)
\end{eqnarray}

\noindent And there is a one-to-one correspondence between $(\beta_1, \beta_2)$ and $(A, \phi)$:

\begin{eqnarray}
\label{eq:aptobeta}
	A &=& \sqrt{\beta_1^2 + \beta_2^2} \nonumber \\
	\phi &=& \arctan(\frac{-\beta_2}{\beta_1})
\end{eqnarray}

\noindent With this substitution, we can re-write  $J(A, \phi)$ with $J(\beta_1, \beta_2)$

\begin{eqnarray}
\label{eq:localbetas}
	J(\beta_1, \beta_2) &=& \sum_{t=0}^{N-1} (x_t - \beta_1 \cos(\frac{2 \pi F t}{N})  \mathbb{1}_{S, S+L-1}(t) - \beta_2 \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S, S+L-1}(t))^2
\end{eqnarray}

And now Equation \ref{eq:localbetas} is multiple linear regression. This means we can solve for $\hat{\beta}_1, \hat{\beta}_2$ analytically. To make thing simpler, we switch to matrix-vector notation with the following definitions

\begin{eqnarray}
	{\bf c} &=& [\cos(\frac{2 \pi F \cdot 0}{N}) \mathbb{1}_{S,S+L-1}(0),  \cos(\frac{2 \pi F \cdot 1}{N}) \mathbb{1}_{S,S+L-1}(1), \ldots \cos(\frac{2 \pi F \cdot (N-1)}{N}) \mathbb{1}_{S,S+L-1}(N-1)]^T \nonumber \\
	{\bf s} &=& [\sin(\frac{2 \pi F \cdot 0}{N}) \mathbb{1}_{S,S+L-1}(0),  \sin(\frac{2 \pi F \cdot 1}{N}) \mathbb{1}_{S,S+L-1}(1), \ldots \sin(\frac{2 \pi F \cdot (N-1)}{N}) \mathbb{1}_{S,S+L-1}(N-1)]^T \nonumber \\
	{\bf U} &=& [{\bf c}, {\bf s}] \nonumber \\
	{\bf x} &=& [x_0, x_1, \ldots, x_{N-1}] \nonumber \\
	\beta &=& [\beta_1, \beta_2]
\end{eqnarray}

\noindent $J(\beta_1, \beta_2)$ can now be written as

\begin{eqnarray}
	\label{eq:sumsquares}
	J(\beta_1, \beta_2) &=& ({\bf x} - {\bf U} {\bf \beta})^T ({\bf x} - {\bf U} {\bf \beta}) 
\end{eqnarray}

\noindent And our estimate for $\hat{\beta}$ is

\begin{eqnarray}
\label{eq:betahat}
	\hat{\beta} &=& ({\bf U}^T {\bf U})^{-1} {\bf U}^T {\bf x} 
\end{eqnarray}

\noindent Which transform into $\hat{A}$ and $\hat{\phi}$ using Equation \ref{eq:aptobeta}. For $\sigma$, we have:

\begin{eqnarray}
	\frac{\partial \ell}{\partial \sigma} &=& \frac{-N}{\sigma} + \frac{1}{\sigma^3} \sum_{t=S}^{S+L-1} (x_t - g_t)^2
\end{eqnarray}

\noindent Setting this to $0$ and plugging in $\hat{A}$ and $\hat{\phi}$ gives:

\begin{eqnarray}
	\hat{\sigma} &=& \sqrt{\frac{1}{N} \sum_{t=0}^{N-1} (x_t - g_t)^2}
\end{eqnarray}

\noindent Similar to standard estimates of $\sigma$ with normal errors. 	

In sum, if S, L, and F are known, we can estimate $\hat{A}$, $\hat{\phi}$, and $\hat{\sigma}$ analytically. But in practice, we don't know S, L, and F, and the next section covers how to estimate them using the Spectrogram. 

\subsection{Estimating S, L, and F}
\label{sec:slf}
Section \ref{sec:aphisigma} shows that, if we {\it knew} S, L, and F, then we could analytically determine the maximum likelihood estimates of A, $\phi$, and $\sigma$. So the next question is, how can we estimate S, L, and F? To answer this, recall the quadratic term we minimized in the previous section (Equation \ref{eq:sumsquares})

\begin{eqnarray}
	J(\hat{\beta_1}, \hat{\beta_2}) &=& ({\bf x} - {\bf U} \hat{\beta})^T ({\bf x} - {\bf U} \hat{\beta})
\end{eqnarray}

\noindent Since {\bf U} includes S, L, and F, we can extend this minimization to include these three parameters

\begin{eqnarray}
\label{eq:extendmin}
	J(\hat{\beta_1}, \hat{\beta_2}, S, L, F) &=& ({\bf x} - {\bf U} \hat{\beta})^T ({\bf x} - {\bf U} \hat{\beta})
\end{eqnarray}

\noindent Plugging in Equation \ref{eq:betahat} gives (\cite{kay1993fundamentals} Equation 7.65):

\begin{eqnarray}
	J(\hat{\beta_1}, \hat{\beta_2}, S, L, F) &=& {\bf x}^T ({\bf I} - {\bf U} ({\bf U}^T {\bf U})^{-1} {\bf U}^T) {\bf x}
\end{eqnarray}

\noindent This is equivalent to finding the values of S, L, and F that maximize 

\begin{eqnarray}
\label{eq:slfmaximization}
	\argmax_{S, L, F} && {\bf x}^{T} {\bf U} ({\bf U}^T {\bf U})^{-1} {\bf U}^T {\bf x}
\end{eqnarray}

\noindent Since ${\bf U} = [{\bf c}, {\bf s}]$, we can re-write Equation \ref{eq:slfmaximization} as

$$
\argmax_{S, L, F}
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}^T
\begin{bmatrix}
{\bf c}^T {\bf c} & {\bf c}^T {\bf s} \\ 
{\bf s}^T {\bf c} & {\bf s}^T {\bf s}
\end{bmatrix}^{-1}
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}
$$

\noindent We can approximate the diagonal terms in the $2 \times 2$ matrix as

\begin{eqnarray}
\label{eq:diagnonalapprox}
	{\bf c}^T {\bf c} &=& \sum_{t=0}^{N-1} \cos(\frac{2 \pi F t}{N})^2 \mathbb{1}_{S, S+L-1}(t) \approx \frac{L}{2} \nonumber \\
	{\bf s}^T {\bf s} &=& \sum_{t=0}^{N-1} \sin(\frac{2 \pi F t}{N})^2 \mathbb{1}_{S, S+L-1}(t) \approx \frac{L}{2} \nonumber 
\end{eqnarray}

\noindent For the off diagonal terms, we have:

\begin{eqnarray}
\label{eq:offdiags}
	{\bf c}^T {\bf s} &=& {\bf s}^T {\bf c} \nonumber \\
	&=& \sum_{t=0}^{N-1} \cos(\frac{2 \pi F t}{N}) \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t) \nonumber \\
	&=& 2 \sum_{t=0}^{N-1} \sin(\frac{4 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t) \nonumber \\
\end{eqnarray}

\begin{figure}[!ht]
  \centering
  \includegraphics[width = 12cm, scale=1]{images/off-diagonal.png}
  \caption{Off Diagonal Terms. N=256, L=10-200, S=30, F=16,28,20. }
\label{fig:offdiagonal}
\end{figure}

\noindent This is actually the key difference between our problem and the example in \cite{kay1993fundamentals}. In \cite{kay1993fundamentals}, there is no indicator term inside the summation, and since this summation covers a sine function over a period, Equation \ref{eq:offdiags} is $\approx$ 0. In our problem, the off-diagonal terms only cancel out when $L$ has a complete number of cycles (see Figure \ref{fig:offdiagonal} for a demonstration), so we need to leave it in the expression. 

For notational purposes, let $z(N, F, S, L)$ be the off-diagonal term. To be concise, we call this $z$ for the remainder of the section. Then we have

$$
\approx 
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}^T
\begin{bmatrix}
\frac{L}{2} & z \\ 
z & \frac{L}{2}	
\end{bmatrix}^{-1}
\begin{bmatrix}
{\bf c}^T {\bf x} \\
{\bf s}^T {\bf x} 
\end{bmatrix}
$$

\noindent The inverse of the $2 \times 2$ matrix is approximately

$$
\frac{1}{\frac{L^2}{4} - z^2}
\begin{bmatrix}
\frac{L}{2} & -z \\
-z & \frac{L}{2}
\end{bmatrix}
\approx 
\frac{4}{L^2}
\begin{bmatrix}
\frac{L}{2} & -z \\
-z & \frac{L}{2}
\end{bmatrix}
$$

\noindent Plugging this back in gives 

\begin{eqnarray}
\label{eq:matrixslf}
	\frac{4}{L^2}[\frac{L}{2} ({\bf c}^T {\bf x})^2 + \frac{L}{2} ({\bf s}^T {\bf x})^2 - 2 z ({\bf c}^T {\bf x}) ({\bf s}^T {\bf x})] &=& \frac{2}{L}[({\bf c}^T {\bf x})^2 + ({\bf s}^T {\bf x})^2] - \frac{8z}{L^2} ({\bf c}^T {\bf x}) ({\bf s}^T {\bf x}) \nonumber 
\end{eqnarray}

\noindent Since the first term on the RHS is normalized by $1/L$, and the second term is normalized by $\frac{1}{L^2}$, the first term dominates this maximization (and if $z = 0$, or close to it, we know that the second term on the RHS goes away). Another way to think about this is that ${\bf c}^T {\bf x}$, when viewed as ``the correlation between {\bf c} and {\bf x}'', would be maximized when ${\bf x} = {\bf c}$, and similarly ${\bf s}^T {\bf x}$ would be maximized when ${\bf s} = x$. Then using Equation \ref{eq:diagnonalapprox}, we can bound the term by

\begin{eqnarray}
\label{eq:diagbound}
	\frac{8z}{L^2} ({\bf c}^T {\bf x}) ({\bf s}^T {\bf x}) &=& 8z (\frac{{\bf c}^T {\bf x}}{L}) (\frac{{\bf s}^T {\bf x}}{L}) \nonumber \\
	&\leq& 2z \nonumber \\
	&\leq& 4
\end{eqnarray}

In any case, I am arguing that the second term in Equation \ref{eq:matrixslf} is negligible, and we can focus on maximizing the first term. The key insight is that we can connect the first term to the Spectrogram. Notice that

\begin{eqnarray}
\label{eq:maxslfspec}
	\frac{2}{L} [ ({\bf c}^T {\bf x})^2 + ({\bf s}^T {\bf x})^2] &=& \frac{2}{L} [ (\sum_{t=0}^{N-1} x_t \cos(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t))^2 + (\sum_{t=0}^{N-1} x_t \sin(\frac{2 \pi F t}{N}) \mathbb{1}_{S,S+L-1}(t))^2]  \nonumber \\
	&=& \frac{2}{L} |\sum_{t=0}^{N-1} x_t \omega_N^{-tF} \mathbb{1}_{S,S+L-1}(t)|^2 \nonumber \\
	&=& \frac{2}{L} |\sum_{t=S}^{S+L-1} x_t \omega_N^{-tF}|^2
\end{eqnarray}

This is strikingly similar to the Periodogram (which is the MLE estimate in \cite{kay1993fundamentals}), the only difference is that the summation indices are different. To see how this connects with the Spectrogram, recall the definition of the Spectrogram computed using a Sliding Window Discrete Fourier Transform (SWDFT) with length $n$ windows

\begin{eqnarray}
\label{eq:swdft}
	a_{k, p, n} &=& \sum_{j=0}^{n-1} x_{p-n+1+j} \omega_n^{-jk} \nonumber \\
	k &=& 0, 1, \ldots, n - 1 \nonumber \\
	p &=& n - 1, n, \ldots N - 1 
\end{eqnarray}

\noindent And if we let $\frac{f}{L} = \frac{F}{N}$, we can interpret Equation \ref{eq:maxslfspec} as a SWDFT with length $L$ windows

\begin{eqnarray}
\label{eq:optimizationslf}
	\argmax_{S, L, F} && \frac{2}{L} |\sum_{t=S}^{S+L-1} x_t \omega_L^{-tf}|^2 
\end{eqnarray}

\noindent Equation \ref{eq:optimizationslf} tells us that our estimates of S, L, and F will be the values that maximize the squared modulus of SWDFT coefficients across varying window sizes. 

So, how would we make these estimates in practice? Notice that if we find the maximum squared modulus SWDFT coefficient across window sizes

\begin{eqnarray}
\label{eq:maxswdftwindow}
	(\hat{k}, \hat{p}, \hat{n}) &=&\argmax_{k,p,n} |a_{k, p, n}|^2
\end{eqnarray}

\noindent We can translate these into estimates of S, L, and F

\begin{eqnarray}
	\hat{S} &=& \hat{p} - \hat{n} + 1 \nonumber \\
	\hat{L} &=& \hat{n} \nonumber \\
	\hat{F} &=& \frac{\hat{k}}{\hat{n}}
\end{eqnarray}

\noindent Our overall estimation approach is summarized in Algorithm \ref{alg:localcosine}

\begin{algorithm}
\caption{MLE Estimation of Bursts Model} 
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$x$, a length $N$ signal} 
\SetAlgoLined
- ($\hat{k}, \hat{p}, \hat{n}) = \argmax_{k, p, n} |a_{k, p, n}|^2$ \\
- ($\hat{S}, \hat{L}, \hat{F}) = f(\hat{k}, \hat{p}, \hat{n})$ \\
- Analytic solutions for $\hat{A}, \hat{\phi}, \hat{\sigma}$ using Section \ref{sec:aphisigma} \\
\Output{MLE estimates $\hat{\theta} = [\hat{A}, \hat{\phi}, \hat{F}, \hat{S}, \hat{L}, \hat{\sigma}]$} 
\label{alg::localcosine}
\end{algorithm}

The computationally intensive part of this approach is repeatedly computing the SWDFT of varying window sizes. We are currently looking into algorithms to speed this up. Finally, further numerical optimization may be performed around the $\hat{F}$, which allows us to search beyond the Fourier Frequencies. 

\section{Results}
\label{sec:results}

\section{Discussion}
\label{sec:discussion}

\bibliographystyle{apa}
\bibliography{references}

\end{document}